<!DOCTYPE html>
<html lang="fr">
  <head>
    <!-- Basic -->
    <meta http-equiv="Content-Type" content="charset=utf-8" />
    <title>WELCOME</title>
    <meta name="description" content="Site du projet du groupe lcd" />
    <meta name="keywords" content="projet encadré PPE" />
    <!-- Mobile Specific Metas -->
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, maximum-scale=1"
    />
    <!-- CSS -->
    <script
      type="module"
      src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.esm.js"
    ></script>
    <script
      nomodule
      src="https://unpkg.com/ionicons@5.5.2/dist/ionicons/ionicons.js"
    ></script>
    <link rel="stylesheet" href="static/css/glitche-basic.css" />
    <link rel="stylesheet" href="static/css/style.css" />
    <link rel="stylesheet" href="static/css/glitche-layout.css" />
    <link rel="stylesheet" href="static/css/magnific-popup.css" />
    <link rel="stylesheet" href="static/css/animate.css" />
    <link rel="stylesheet" href="static/css/monokai-sublime.min.css" />
    <script src="static/js/highlight.min.js"></script>

    <script>
      hljs.initHighlightingOnLoad();
    </script>

    <link
      href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i|Nunito:300,300i,400,400i,600,600i,700,700i|Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Merriweather"
      rel="stylesheet"
    />

    <link rel="shortcut icon" href="icon.ico" />
  </head>
  <body>
    <!-- Preloader -->
    <div class="preloader">
      <div class="centrize full-width">
        <div class="vertical-center">
          <div class="pre-inner">
            <div class="load typing-load"><p>chargement...</p></div>
            <span class="typed-load"></span>
          </div>
        </div>
      </div>
    </div>
    <!-- Container -->
    <div class="container">
      <!-- Header -->
      <header>
        <div class="head-top">
          <nav class="dropdownmenu" style="position: relative">
            <ul>
              <li><a href="nous.html" class="lnk">Nous</a></li>
              <li>
                <a href="blog.html" class="lnk">Blog</a>
              </li>
              <li style="width: 140px">
                <a href="scripts.html"
                  ><span>Scripts </span
                  ><ion-icon name="caret-down-outline"></ion-icon>
                </a>
                <ul id="submenu">
                  <li><a href="scripts.html#partie1">Dictionnaire</a></li>
                  <li><a href="scripts.html#partie2">Modules</a></li>
                  <li><a href="scripts.html#partie3">Main.py</a></li>
                  <li><a href="scripts.html#partie4">LDA</a></li>
                </ul>
              </li>
              <li>
                <a href="produits.html"
                  ><span>Produits </span
                  ><ion-icon name="caret-down-outline"></ion-icon>
                </a>
                <ul id="submenu">
                  <li><a href="produits.html#xml">XML</a></li>
                  <li><a href="produits.html#json">js</a></li>
                  <li><a href="produits.html#lda">LDA</a></li>
                </ul>
              </li>
              <li style="width: 140px">
                <a href="projet.html"
                  ><span>Projet </span
                  ><ion-icon name="caret-down-outline"></ion-icon>
                </a>
                <ul id="submenu">
                  <li><a href="projet.html#presentation">Présentation</a></li>
                  <li><a href="projet.html#bao1">BàO1</a></li>
                  <li><a href="projet.html#bao1">BàO2</a></li>
                  <li><a href="projet.html#bao3">BàO3</a></li>
                  <li><a href="projet.html#bao4">BàO4</a></li>
                </ul>
              </li>
              <li><a href="index.html">Accueil</a></li>
            </ul>
          </nav>
          <a href="#" class="menu-btn"></a>
          <div class="top-menu"></div>
        </div>
      </header>
      <!-- Wrapper -->
      <div class="wrapper">
        <!-- Started -->
        <div class="section started">
          <div class="centrize full-width">
            <div class="vertical-center">
              <div class="started-content">
                <div class="h-title">Les <span>Scripts</span></div>
                <div class="h-subtitle typing-subtitle">
                  <p>de type python, shell.</p>
                  <p>PPE2 pluriTAL.</p>
                </div>
                <span class="typed-subtitle"></span>
              </div>
            </div>
          </div>
          <a href="#" class="mouse_btn"
            ><span class="icon">
              <ion-icon name="arrow-down-outline"></ion-icon> </span
          ></a>
        </div>

        <!-- About -->
        <div class="section about">
          <div class="content">
            <div class="title">
              <div class="title_inner5">Les scripts</div>
            </div>
            <div class="image">
              <img src="static/picture/script1.png" alt="" />
            </div>
            <div class="desc">
              <h4
                style="
                  font-family: system-ui, -apple-system, BlinkMacSystemFont,
                    'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans',
                    'Helvetica Neue', sans-serif;
                "
              >
                Les scripts se divisent principalement en 4 parties:
              </h4>
              <div style="line-height: 20px;">
                <li>Le script utilisé pour la création du dictionnaire(Petit exercice jusqu'à la semaine 3)</li>
                </li>
                <li>Scripts de modules, de fonctions et de classes</li>
                <li>Script main (qui permet de lancer les scripts précédents)</li>
                <li>Script LDA (qui permet de faire le traitement LDA et de générer les visualisations associées))</li>
            </div>
              
            </div>
            <div class="clear"></div>
            <br />
            <br />
            <div class="clear"></div>
            <br />
            <br />
          </div>
        </div>

        <div class="section scripts">
          <div class="content">
            <div class="cols">
              <!-- Script1 -->
              <div id="partie1">
                <div class="title">
                  <div class="title_inner" style="text-transform: none">
                    Le script utilisé pour la création du dictionnaire
                  </div>
                  <p>
                    Ce script est une intégration des réalisations des trois
                    premières semaines de travail, qui permet de traiter des
                    fichiers texte. Il utilise des arguments de ligne de
                    commande pour spécifier les chemins des fichiers.
                  </p>
                  <p>
                    L'argument
                    <i style="background-color: antiquewhite">--chemin</i> est
                    utilisé pour spécifier les chemins des fichiers. Il définit
                    des fonctions
                    <span style="color: orangered">
                      construit_liste_de_chaines</span
                    >,
                    <span style="color: rgb(10, 170, 101)"
                      >construit_liste_de_chaines_r2_r3</span
                    >
                    , <span style="color: rgb(33, 5, 215)">count_words</span> ,
                    <span style="color: rgb(190, 17, 161)">doc_freq</span> et
                    <span style="color: rgb(135, 131, 0)">lexique</span> qui
                    permettent de compter la fréquence des mots et la fréquence
                    des documents, et de générer un lexique.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
###################################### Partie r1
import sys
import argparse
parser = argparse.ArgumentParser(
    description="Obtenir un argument par bash command, un argument qui indique le chemin des fichiers")
parser.add_argument('--chemin', type=str, nargs='+',
                    help='le chemin relatif des fichiers')
args = parser.parse_args()

def construit_liste_de_chaines(corpus_path):
    list_contenu = []
    for txtname in (corpus_path):
        with open(txtname, 'r') as file:
            data = file.read()
            list_contenu.append(data)
    return list_contenu

###################################### Partie r2 & r3
def construit_liste_de_chaines_r2_r3():
    list_contenu = []
    for txtname in sys.stdin:
        if txtname.endswith('.txt'):
            ## r3
            txtname = txtname.replace('\n', '')
            with open(txtname, 'r') as file:
                data = file.read()
                list_contenu.append(data)
        else:
            ## r2
            list_contenu.append(txtname)
    
    return list_contenu


def count_words(corpus_list):
    dictionnaire_freq = {}
    for sing_contenu in corpus_list:
        mots = sing_contenu.split()
        for mot in mots:
            if mot in dictionnaire_freq:
                dictionnaire_freq[mot] += 1
            else:
                dictionnaire_freq[mot] = 1
    return dictionnaire_freq


def doc_freq(corpus):
    nb_doc = {}
    for doc in corpus:
        words = set(doc.split())
        for word in words:
            if word in nb_doc:
                nb_doc[word] += 1
            else:
                nb_doc[word] = 1
    return nb_doc


def lexique():
    liste_gros = []
    if (args.chemin == None):
        # r2 & r3
        liste_gros = construit_liste_de_chaines_r2_r3()
    elif (isinstance(args.chemin, list)):
        # r1
        liste_gros = construit_liste_de_chaines(args.chemin)

    dict_freq = count_words(liste_gros)
    dict_occurence = doc_freq(liste_gros)
    len_liste_final = len(dict_freq)
    for i in range(len_liste_final):
        key = list(dict_freq)[i]
        value1 = list(dict_freq.values())[i]
        value2 = list(dict_occurence.values())[i]
        print("{0:^20}|{1:^20}|{2:^20}".format(key, value1, value2))


if __name__ == '__main__':
    lexique()
                                       
                                  </code></pre>
              </div>

              <!-- Script2 -->

              <div>
                <div id="partie2" style="margin-top: 150px">
                  <h1
                    style="
                      color: rgb(29, 10, 243);
                      font-family: Open Sans;
                      text-transform: none;
                      font-size: 40px;
                      text-align: center;
                    "
                  >
                    <strong>
                      Scripts de modules, de fonctions et de classes</strong
                    >
                  </h1>
                  <p style="font-size: 15px">
                    Il est possible d'utiliser les fonctions déjà définies pour
                    exécuter des tâches ou des opérations spécifiques dans la
                    fonction principale. En séparant ces fonctions de la
                    fonction principale, on peut mieux organiser le code et
                    améliorer sa lisibilité et sa maintenabilité.
                  </p>
                </div>
              </div>
              <div>
                <div class="title" style="margin-top: 70px">
                  <div class="title_inner" style="text-transform: none">
                    1. Scripts de parser : etree et re
                  </div>
                  <p>
                    Ce script contient des fonctions qui sont responsables de
                    l'analyse de fichiers XML et de l'extraction d'informations
                    à partir de texte à l'aide d'expressions régulières. Le
                    script "etree" utilise le module "xml.etree.ElementTree"
                    pour le parsing des fichiers XML, tandis que le script "re"
                    utilise des expressions régulières pour extraire des
                    informations d'un texte. Les deux fonctions peuvent être
                    appelés en tant que modules.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
# parser avec module "etree"                
import xml.etree.ElementTree as ET


# Objectif : extraire et afficher le titre et la description de chaque article dans un fichier XML.

# deuxieme choix : module "etree" avec argparse
# Si le fichier est importé en tant que module, la fonction title_descr peut être appelée directement
def etree_parser(xml):
    tree = ET.parse(xml)
    root = tree.getroot()
    for article in root.findall(".//item"):
        title = article.find('title').text
        description = article.find('description').text
        yield title, description

# ------------------------------------------------------------------------------------------------------------------------------

# parser module "re"
import re
from pathlib import Path

# Objectif : extraire et afficher le titre et la description de chaque article dans un fichier XML.
# Si le fichier est importé en tant que module, la fonction title_descr peut être appelée directement

def re_parser(fic):
    fic_path = Path(fic)
    xml = fic_path.read_text()
    # 1. trouver chaque article comme un élément dans une liste
    art_pattern = re.compile('<item>.*?</item>')
    art_liste = art_pattern.findall(xml)
    # 2. dans chaque article, trouver son titre et description
    for article in art_liste:
        tit_pattern = re.compile(r"<title><!\[CDATA\[(.*?)\]\]></title>")
        titre = tit_pattern.search(article).group(1)
        desc_pattern = re.compile(
            r"<description><!\[CDATA\[(.*?)\]\]></description>")
        description = desc_pattern.search(article).group(1)
        yield titre, description
                                                  </code></pre>
              </div>

              <!-- Script 3 -->
              <div>
                <div class="title" style="margin-top: 200px">
                  <div class="title_inner" style="text-transform: none">
                    2. Dataclass
                  </div>
                  <p>
                    Ce script définit plusieurs classes de données qui
                    permettent de créer des objets avec des attributs
                    pré-définis. Les classes définies dans ce script utilisent
                    le décorateur
                    <span style="color: brown">@dataclass</span> pour créer des
                    objets avec des attributs pré-définis. Ici, ces trois
                    classes permettent de structurer les données d'un corpus
                    textuel avec des articles, des analyses linguistiques et des
                    informations associées. Elles facilitent la manipulation et
                    l'accès aux données, ainsi que la gestion de la structure
                    hiérarchique des données.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
from typing import List, Optional
from dataclasses import dataclass
from pathlib import Path


@dataclass
class Analyse:
    form: str
    lemma: str
    pos: str


@dataclass
class Article:
    title: str
    desc: str
    date: str
    categorie: str
    analyse: Optional[List[Analyse]] = None


@dataclass
class Corpus:
    cat: List[str]
    start: str
    end: str
    path: Path
    content: List[Article]                           
                                  </code></pre>
              </div>

              <!-- Script4 -->
              <div>
                <div class="title" style="margin-top: 200px">
                  <div class="title_inner" style="text-transform: none">
                    3. Exports
                  </div>

                  <p>
                    <span style="color: darkblue">write_json</span> : Cette
                    fonction prend un objet Corpus et un chemin de destination
                    en paramètres. Elle utilise la fonction asdict pour
                    convertir l'objet Corpus en un dictionnaire Python, puis
                    utilise json.dump pour écrire ce dictionnaire dans un
                    fichier JSON. Cela permet de sauvegarder les données des
                    classes dataclass dans un format JSON.
                  </p>

                  <p>
                    <span style="color: chocolate">article_to_xml</span> : Cette
                    fonction prend un objet Article en paramètre et renvoie un
                    élément XML correspondant à cet article. Elle crée un
                    élément "article" et attribue les valeurs des attributs date
                    et categorie. Elle crée également des éléments "title",
                    "description" et "analyse" avec leurs valeurs respectives.
                    Si l'article contient des analyses, elle crée des éléments
                    "token" pour chaque analyse avec les attributs "form",
                    "lemma" et "pos". Ensuite, elle renvoie l'élément XML de
                    l'article.
                  </p>

                  <p>
                    <span style="color: purple">write_xml</span> : Cette
                    fonction prend un objet Corpus et un chemin de destination
                    en paramètres. Elle crée un élément "corpus" et attribue les
                    valeurs des attributs start et end. Elle crée également un
                    élément "categories" et un élément "content". Pour chaque
                    catégorie du corpus, elle crée un élément "cat" sous
                    l'élément "categories". Pour chaque article du corpus, elle
                    utilise la fonction article_to_xml pour obtenir l'élément
                    XML correspondant, puis l'ajoute à l'élément "content".
                    Enfin, elle crée un objet ElementTree à partir de l'élément
                    "corpus", applique l'indentation et écrit le résultat dans
                    un fichier XML à l'aide de la méthode write.
                  </p>

                  <p>
                    Ces fonctions permettent de convertir les données des
                    classes dataclass en formats JSON et XML pour la sauvegarde
                    ou l'échange de données.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
import json
from dataclasses import asdict
from xml.etree import ElementTree as ET
from datastructures import Corpus, Article, Analyse


# fonction pour sortir les données de dataclasses en json
def write_json(corpus: Corpus, destination: str):
    with open(destination, "w", encoding='utf-8') as fout:
        json.dump(asdict(corpus), fout, ensure_ascii=False, indent=2)

# fonction pour sortir les données de dataclasses en xml

def article_to_xml(article: Article) -> ET.Element:
    xml_article = ET.Element("article")
    xml_article.attrib['date'] = article.date
    xml_article.attrib['categorie'] = article.categorie
    title = ET.SubElement(xml_article, "title")
    description = ET.SubElement(xml_article, "description")
    analyses = ET.SubElement(xml_article, "analyse")
    title.text = article.title
    description.text = article.desc
    for analyse in article.analyse:
        token = ET.SubElement(analyses, "token")
        token.attrib['form'] = analyse.form
        if analyse.pos and analyse.lemma is not None:
            token.attrib['lemma'] = analyse.lemma
            token.attrib['pos'] = analyse.pos
        else:
            token.attrib['lemma'] = "null"
            token.attrib['pos'] = "null"
            print(
                f"Attention: l'analyse pos du mot '{analyse.form}' renvoie null, skipping...")
    return xml_article

# fonction pour sortir les données de dataclasses en xml

def write_xml(corpus: Corpus, destination: str):
    root = ET.Element("corpus")
    root.attrib['start'] = corpus.start
    root.attrib['end'] = corpus.end
    categories = ET.SubElement(root, "categories")
    for c in corpus.cat:
        ET.SubElement(categories, "cat").text = c

    content = ET.SubElement(root, "content")
    for article in corpus.content:
        art_xml = article_to_xml(article)
        content.append(art_xml)

    tree = ET.ElementTree(root)
    ET.indent(tree)
    tree.write(destination, encoding='utf-8', xml_declaration=True)                                              
                                  </code></pre>
              </div>

              <!-- Script5 -->
              <div>
                <div class="title" style="margin-top: 200px">
                  <div class="title_inner" style="text-transform: none">
                    4. TAL modules
                  </div>
                  <p>
                    Ces fonctions fournissent un choix de modules NLP différents
                    pour effectuer des analyses et des traitements de texte.
                    Selon les besoins, vous pouvez choisir le module qui
                    convient le mieux. Chaque fonction prend un texte en entrée
                    et renvoie un dictionnaire contenant des informations sur la
                    lemmatisation et l'annotation grammaticale des mots. Ces
                    informations peuvent être utilisées pour des tâches et des
                    analyses LDA ultérieures.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
# arsenal of NLP modules
import spacy
import trankit
import stanza


def trankit_parser():
    return trankit.Pipeline('french', gpu=False)


def trankit_analyse(parser, text: str):
    desc_analyse = {}
    fr_output = parser.posdep(text)
    lemma_doc = parser.lemmatize(text)
    for x in lemma_doc.get('sentences'):
        for token in x.get('tokens'):
            analyse_contenu = []
            analyse_contenu.append(token.get('lemma'))
            for y in fr_output.get('sentences'):
                for token2 in y.get('tokens'):
                    if token2.get('text') == token.get('text'):
                        analyse_contenu.append(token2.get('upos'))
            desc_analyse[token.get('text')] = analyse_contenu
    return desc_analyse


def spacy_parser():
    return spacy.load("fr_core_news_sm")


def spacy_analyse(parser, text: str):
    doc = parser(text)
    desc_analyse = {}
    for token in doc:
        analyse_contenu = []
        analyse_contenu.append(token.lemma_)
        analyse_contenu.append(token.pos_)
        desc_analyse[token.text] = analyse_contenu
    return desc_analyse


def stanza_parser():
    return stanza.Pipeline('fr')


def stanza_analyse(parser, text: str):
    doc = parser(text)
    desc_analyse = {}
    for sent in doc.sentences:
        for word in sent.words:
            analyse_contenu = []
            analyse_contenu.append(word.lemma)
            analyse_contenu.append(word.upos)
            desc_analyse[word.text] = analyse_contenu
    return desc_analyse                                       
                                  </code></pre>
              </div>

              <!-- Script6 -->
              <div id="partie3">
                <div style="margin-top: 150px">
                  <h1
                    style="
                      color: rgb(29, 10, 243);
                      font-family: Open Sans;
                      text-transform: none;
                      font-size: 40px;
                      text-align: center;
                    "
                  >
                    <strong>"main.py" </strong>
                  </h1>
                </div>
              </div>
              <div>
                <div class="title" style="margin-top: 40px">
                  <div class="title_inner" style="text-transform: none">
                    le main script
                  </div>
                  <p>
                    Le script principal parcourt les fichiers XML correspondant
                    à certaines catégories et périodes spécifiées, extrait les
                    titres et descriptions des articles, effectue une analyse
                    linguistique à l'aide d'un module NLP, puis enregistre ou
                    affiche les résultats en fonction des options choisies.
                  </p>
                  <p>
                    Il importe les modules nécessaires et définit des
                    constantes, telles que les mois et les jours, qui seront
                    utilisées ultérieurement. Ensuite, il définit un
                    dictionnaire qui associe des codes de catégories à leurs
                    noms correspondants.
                  </p>
                  <p>
                    Ensuite, le script parcourt les fichiers XML dans un
                    répertoire donné, en tenant compte des critères tels que les
                    catégories, la date de début et la date de fin. Cette
                    fonction renvoie un générateur qui produit un par un les
                    fichiers XML répondant aux critères spécifiés.
                  </p>
                  <p>
                    Pour chaque fichier retourné, il extrait le titre et la
                    description à l'aide de la fonction fonc. Si ces
                    informations sont valides, un objet Article est créé et
                    ajouté au corpus. Le script utilise également un module NLP
                    spécifié pour effectuer une analyse du titre et de la
                    description. Les informations obtenues, telles que les
                    lemmes et les étiquettes de partie du discours, sont
                    associées à chaque mot et ajoutées à l'objet Article. Enfin,
                    en fonction des arguments de ligne de commande, le script
                    écrit les résultats dans un fichier de sortie au format
                    JSON, XML ou pickle, ou les affiche simplement dans la
                    console.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
from title_descr_re import re_parser
from title_descr_etree import etree_parser
import re
from xml.etree import ElementTree as et
import argparse
import sys
from typing import Optional, List, Dict
from datetime import date
from pathlib import Path
import pickle
from datastructures import Corpus, Article, Analyse
from exports import write_json, write_xml
import nlp_modules
from tqdm import tqdm
import gensim_lda_pred as lda


# Objectif : parcourir les fichiers et, extraire et afficher le titre et la description de chaque article correspondant à une catégorie
MONTHS = ["Jan",
          "Feb",
          "Mar",
          "Apr",
          "May",
          "Jun",
          "Jul",
          "Aug",
          "Sep",
          "Oct",
          "Nov",
          "Dec"]

DAYS = [f"{x:02}" for x in range(1, 32)]


cat_dict = {
    'une': '0,2-3208,1-0,0',
    'international': '0,2-3210,1-0,0',
    'europe': '0,2-3214,1-0,0',
    'societe': '0,2-3224,1-0,0',
    'idees': '0,2-3232,1-0,0',
    'economie': '0,2-3234,1-0,0',
    'actualite-medias': '0,2-3236,1-0,0',
    'sport': '0,2-3242,1-0,0',
    'planete': '0,2-3244,1-0,0',
    'culture': '0,2-3246,1-0,0',
    'livres': '0,2-3260,1-0,0',
    'cinema': '0,2-3476,1-0,0',
    'voyage': '0,2-3546,1-0,0',
    'technologies': '0,2-651865,1-0,0',
    'politique': '0,57-0,64-823353,0',
    'sciences': 'env_sciences',
}


def categorie_of_ficname(ficname: str) -&gt; Optional[str]:
    for nom, code in cat_dict.items():
        if code in ficname:
            return nom
    return None


def convert_month(mon: str) -&gt; int:
    m = MONTHS.index(mon) + 1
    return m

    # Etape 2 : filtrer les fichiers selon les exigences, répondant aux critères suivants  :
    # 1. les fichiers sont au format XML, placés dans un dossier Corpus/Mmm/JJ/19-00-00/
    # 2. les fichiers sont de bonne période et de bonne(s) catégorie(s).
    # 3. les fichiers XML comme "fil1646762506-v1.xml" doivent être exclus. Les fichiers attendus doivent avoir leur code de categorie dans leur nom

    # La fonction 'parcours_path' prend en entrée un répertoire corpus_dir et plusieurs arguments optionnels
    # Elle itère sur chaque fichier XML dans le répertoire et renvoie un générateur qui produit un par un les fichiers XML qui répondent aux critères spécifiés.


def parcours_path(corpus_dir: Path, categories: Optional[List[str]] = None, start_date: Optional[date] = None, end_date: Optional[date] = None):
    if categories is not None and len(categories) &gt; 0:
        # 对于所有在categories列表里的c，我们依次在cat-code里找出其对应的答案
        categories = [cat_dict[c] for c in categories]
    else:
        categories = cat_dict.values()  # on prend tout

    for month_dir in corpus_dir.iterdir():
        if month_dir.name not in MONTHS:
            # on ignore les dossiers qui ne sont pas des mois
            continue  # 跳出这次循环，进行下一个month_dir的判断
        m_num = convert_month(month_dir.name)  # 对于Jan,我们将其对应到数字形式 m_num = 1
        for day_dir in month_dir.iterdir():
            if day_dir.name not in DAYS:
                # on ignore les dossiers qui ne sont pas des jours    2022-07-21  2022-09-13
                continue
            # selon le format "2022-01-25",on 生成对应的date对象
            d = date.fromisoformat(f"2022-{m_num:02}-{day_dir.name}")
            if (start_date is None or start_date &lt;= d) and (end_date is None or end_date &gt;= d):  # 保证该日期符合在开始和结束日期之间。
                for time_dir in day_dir.iterdir():
                    if re.match(r"\d\d-\d\d-\d\d", time_dir.name):  # 对应19-00-00
                        for fic in time_dir.iterdir():
                            # 进一步过滤xml文件的名称
                            if fic.name.endswith(".xml") and any([c in fic.name for c in categories]):
                                # un générateur qui produit un par un les fichiers XML qui répondent aux critères spécifiés.
                                c = categorie_of_ficname(fic.name)
                                yield(fic, str(d), c)


if __name__ == "__main__":
    # Etape 1 : obtenir les fichiers d'après l'input
    parser = argparse.ArgumentParser(
        description="le script sert à extraire et afficher le titre et la description de chaque article dans un fichier XML")
    parser.add_argument(
        "-m", help="méthode de parsing (etree ou re)", default="etree")
    parser.add_argument(
        "-s", help="start date (iso format)", default="2022-06-15")
    parser.add_argument("-e", help="end date (iso format)",
                        default="2022-09-01")
    parser.add_argument("-o", help="output file (stdout if not specified")
    parser.add_argument(
        "corpus_dir", help="root dir of the corpus data,qui contient des dossiers/fichiers xml")
    parser.add_argument("categories", nargs="*",
                        help="la ou les catégories des fichiers XML désirés")
    args = parser.parse_args()
    if args.m == 'etree':
        fonc = etree_parser
        print("vous avez choisi etree pour parser")
    elif args.m == 're':
        fonc = re_parser
        print("vous avez choisi re pour parser")
    else:
        print("méthode non disponible", file=sys.stderr)
        sys.exit()
    # f = un fichier xml, obtenu par yield, soit le "yield(fic)"

    # creation du corpus
    tk_parser = nlp_modules.create_parser()
    print('l\'analyse commence, veuillez patienter...')
    corpus = Corpus(args.categories, args.s, args.e, args.corpus_dir, [])
    for f, dt, c in tqdm(parcours_path(Path(args.corpus_dir),
                                        start_date=date.fromisoformat(args.s),
                                        end_date=date.fromisoformat(args.e),
                                        categories=args.categories)):
        for title, desc in tqdm(fonc(f)):
            if title and desc is not None:
                article = Article(title, desc, dt, c, [])
                corpus.content.append(article)
                analyse_dict = nlp_modules.trankit_analyse(
                    tk_parser, title + " " + desc)
                for forme in analyse_dict:
                    token = Analyse(forme, analyse_dict.get(forme)[
                        0], analyse_dict.get(forme)[1])
                    article.analyse.append(token)
      # d'après le format de fichier de sortie, on écrit le résultat dans le fichier correspondant
    if args.o.endswith(".js"):
        print('parsing done, outputed in the file de format json you required')
        write_json(corpus, args.o)

    # output xml
    elif args.o.endswith(".xml"):
        print('parsing done, outputed in the xml file you required')
        write_xml(corpus, args.o)

    elif args.o.endswith(".pickle"):
        print(
            'parsing done, outputed in the file pickle you required, may not that visible')
        with open(args.o, 'wb') as f:
            pickle.dump(corpus, f)
        with open(args.o, 'rb') as r:
            returned_data = pickle.load(r)

    else:
        print('parsing done, outputed in the terminal')
        for f in parcours_path(Path(args.corpus_dir),
                                start_date=date.fromisoformat(args.s),
                                end_date=date.fromisoformat(args.e),
                                categories=args.categories):
            for title, desc in fonc(f):
                if title and desc is not None:
                    print("&gt;&gt;&gt; Titre:", title)
                    print("&gt;&gt;&gt; Description:", desc)
                    print(
                        '------------------------------------------------------------------------------------------------------------------------------')
              </code></pre>
              </div>

              <!-- Script7 -->
              <div id="partie4">
                <div class="title" style="margin-top: 200px">
                  <div class="title_inner" style="text-transform: none">
                    LDA Topic Modeling
                  </div>
                  <p>
                    Ce script charge un corpus de documents à partir d'un
                    fichier XML, JSON ou pickle, prétraite les données en
                    supprimant les mots vides et en identifiant les bigrammes,
                    crée un modèle LDA à partir des documents prétraités, évalue
                    la cohérence des sujets générés et génère une visualisation
                    interactive des sujets.
                  </p>

                  <p>
                    Le script effectue une étape de nettoyage des données en
                    supprimant les mots vides (stopwords) du corpus, en
                    utilisant une liste de mots vides prédéfinie pour le
                    français, ainsi qu'une liste personnalisée provenant d'un
                    fichier texte. Ensuite, le script identifie les bigrammes
                    dans les documents à l'aide du module Phrases de gensim.
                  </p>
                  <p>
                    Ensuite, le script crée un dictionnaire à partir des
                    documents prétraités et filtre les termes extrêmes (trop
                    fréquents ou trop rares) du dictionnaire. Le script
                    convertit ensuite les documents prétraités en une
                    représentation de type "sac de mots" (bag-of-words) à l'aide
                    du dictionnaire créé précédemment.
                  </p>

                  <p>
                    Après les opérations sur le dataset, le modèle LDA est
                    entraîné en utilisant les paramètres spécifiés tels que le
                    nombre de sujets, la taille du lot (chunksize), le nombre de
                    passages (passes) et le nombre d'itérations. Après
                    l'entraînement, le script évalue la cohérence des sujets
                    générés à l'aide de la mesure de cohérence des sujets et
                    affiche les sujets triés par ordre de cohérence.
                  </p>

                  <p>
                    Enfin, le script génère une visualisation interactive des
                    sujets à l'aide de la bibliothèque pyLDAvis, et sauvegarde
                    la visualisation sous la forme d'un fichier HTML. En résumé,
                    ce script charge un corpus de documents à partir d'un
                    fichier XML, JSON ou pickle, prétraite les données en
                    supprimant les mots vides et en identifiant les bigrammes,
                    crée un modèle LDA à partir des documents prétraités, évalue
                    la cohérence des sujets générés et génère une visualisation
                    interactive des sujets.
                  </p>
                </div>
                <div class="date">Script</div>
                <pre
                  class="script Python"
                ><code style="font-family: 'Courier New', Courier, monospace;">
import pickle
from pprint import pprint
from gensim.models import LdaModel
from gensim.corpora import Dictionary
from gensim.models import Phrases
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.corpus import stopwords
import json
from xml.etree import ElementTree as ET
import argparse
import logging
import pyLDAvis.gensim_models
import sys
logging.basicConfig(
    format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)


def load_file_xml(xml_file, type, pos=[]):
    tree = ET.parse(xml_file)
    root = tree.getroot()
    # corpus
    corpus = []
    for analyse in root.iter('analyse'):
        single_article = []
        for token in analyse.iter('token'):
            if pos == []:
                token_1 = token.get(type)  # type = lemma / forme
                if token_1 is not None:
                    single_article.append(token_1)
            else:
                if token.get('pos') in pos:
                    token_1 = token.get(type)
                    if token_1 is not None:
                        single_article.append(token_1)
        corpus.append(single_article)
    return corpus


def load_file_json(json_file, type, pos=[]):
    with open(json_file, 'r') as f:
        data = json.load(f)
    corpus = []
    for article in data['content']:
        single_article = []
        for token in article['analyse']:
            if pos == []:
                token_1 = token[type]
                if token_1 is not None:
                    single_article.append(token_1)
            elif token.get('pos') in pos:
                token_1 = token[type]
                if token_1 is not None:
                    single_article.append(token_1)
        corpus.append(single_article)
    return corpus


def load_file_pickle(pickle_file, type, pos=[]):
    with open(pickle_file, 'rb') as f:
        data = pickle.load(f)
    corpus = []
    for article in data.content:
        single_article = []
        for token in article.analyse:
            if pos == []:
                token_1 = token.__getattribute__(type)
                if token_1 is not None:
                    single_article.append(token_1)
            elif token.pos in pos:
                token_1 = token.__getattribute__(type)
                if token_1 is not None:
                    single_article.append(token_1)
        corpus.append(single_article)
    return corpus


# stopwords
def stop_words(corpus):
    with open('stopwords-fr.txt', 'r', encoding='utf-8') as f:
        sw_file = set(f.read().split())
    stop_words = set(stopwords.words('french')).union(sw_file)
    print(stop_words)
    corpus = [[word for word in doc if word not in stop_words]
              for doc in corpus]
    return corpus


def bigram(docs):
    bigram = Phrases(docs, min_count=10)
    for idx in range(len(docs)):
        for token in bigram[docs[idx]]:
            if '_' in token:
                # Token is a bigram, add to document.
                docs[idx].append(token)
    return docs


# Create a dictionary representation of the documents.
def filter_extremes(docs):
    dictionary = Dictionary(docs)
    dictionary.filter_extremes(no_below=5, no_above=0.9)
    return dictionary


# Bag-of-words representation of the documents.
def doc2bow(docs, dictionary):
    corpus = [dictionary.doc2bow(doc) for doc in docs]
    return corpus


###############################################################################
# Training
# Set training parameters.
def train_lda_model(corpus, dictionary, num_topics=20, chunksize=2000, passes=20, iterations=400, eval_every=None):
    # Make a index to word dictionary.
    temp = dictionary[0]  # This is only to "load" the dictionary.
    id2word = dictionary.id2token
    model = LdaModel(
        corpus=corpus,
        id2word=id2word,
        chunksize=chunksize,
        alpha='auto',
        eta='auto',
        iterations=iterations,
        num_topics=num_topics,
        passes=passes,
        eval_every=eval_every
    )
    return model

###############################################################################
# average topic coherence and print the topics in order of topic coherence.
def topic_coherence(model, corpus, num_topics=20):
    top_topics = model.top_topics(corpus)
    avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics
    print('Average topic coherence: %.4f.' % avg_topic_coherence)
    pprint(top_topics)
    return avg_topic_coherence, top_topics

# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.


###############################################################################

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Ce script permet de créer un modèle LDA à partir d'un corpus, et de prédire les topics d'un nouveau document")
    parser.add_argument("-s", help="input file")
    parser.add_argument(
        "-m", choices=['xml', 'json', 'pickle'], help='Type of input file')
    parser.add_argument(
        "-t", help="choisir le lemma ou la forme")
    parser.add_argument(
        "pos", nargs="*", help="determiner les pos (une liste)")
    parser.add_argument("-n", help="num_topics", default=10)
    parser.add_argument("-c", help="chunksize", default=2000)
    parser.add_argument("-p", help="passes", default=20)
    parser.add_argument("-i", help="iterations", default=400)
    args = parser.parse_args()
    if args.s:
        if args.m == 'xml':
            corpus = load_file_xml(args.s, args.t, args.pos)
        elif args.m == 'json':
            corpus = load_file_json(args.s, args.t, args.pos)
        elif args.m == 'pickle':
            corpus = load_file_pickle(args.s, args.t, args.pos)
        else:
            print('Invalid filetype')
            sys.exit(1)
        corpus = stop_words(corpus)
        corpus = [[token for token in doc if not token.isnumeric()]
                  for doc in corpus]
        docs_c = [[token for token in doc if len(token) &gt; 1] for doc in corpus]
        bi_docs = bigram(docs_c)
        dictionnaire = filter_extremes(bi_docs)
        print(dictionnaire)
        doc_bow = doc2bow(bi_docs, dictionnaire)
        print('Number of unique tokens: %d' % len(dictionnaire))
        print('Number of documents: %d' % len(doc_bow))
        num_topics = args.n
        chunksize = args.c
        passes = args.p
        iterations = args.i
        eval_every = None
        model = train_lda_model(
            doc_bow, dictionnaire, num_topics=num_topics, chunksize=chunksize, passes=passes, eval_every=eval_every)
        topic_coherence(model, doc_bow, num_topics=num_topics)
        lda_dp = pyLDAvis.gensim_models.prepare(
            model, doc_bow, dictionnaire)
        pyLDAvis.save_html(lda_dp, 'lda_dp.html')                                               
                                  </code></pre>
              </div>
            </div>
          </div>
        </div>
      </div>
      <div class="ctn">
        <a href="#" class="button2 b-blue rot-135"
          >　　　　　　　　　　　　　　　　　　　　　　　　Scripts　　　　　　　　　　　　　　　　　　　　　　　　　　</a
        >

        <br />
        <br />
        <br />
        <br />
        <br />
      </div>

      <!-- Footer -->
      <footer>
        <div class="soc">
          <a target="_blank" href="https://gitlab.com/ppe2023/ppe2_lcd">
            <ion-icon name="logo-gitlab" style="font-size: 25px"></ion-icon>
          </a>
        </div>
        <div class="copy" style="font-family: Open Sans; font-weight: 600">
          Page de présentation du projet PPE2 22/23 réalisé par
          <span style="color: rgb(0, 0, 0)">#le groupe LCD</span>
          <br />

          <a
            style="color: rgb(31, 9, 177)"
            href="http://www.tal.univ-paris3.fr/plurital/"
            ><strong>@pluriTAL</strong></a
          >
        </div>

        <div class="clr"></div>
      </footer>
      <!-- Lines -->
      <div class="line top"></div>
      <div class="line bottom"></div>
      <div class="line left"></div>
      <div class="line right"></div>
    </div>
    <!-- jQuery Scripts -->
    <script src="static/js/jquery.min.js"></script>
    <script src="static/js/jquery.validate.js"></script>
    <script src="static/js/typed.js"></script>
    <script src="static/js/magnific-popup.js"></script>
    <script src="static/js/imagesloaded.pkgd.js"></script>
    <script src="static/js/isotope.pkgd.js"></script>
    <!-- Main Scripts -->
    <script src="static/js/glitche-scripts.js"></script>
  </body>
</html>
