# Semaine 10

### 2023.04.17

**Xiaohua : **

Selon les corrigés du professeur, j'ai apporté quelques modifications à la branche de s9. J'ai remarqué que le professeur utilisait le module Optionnel pour accepter et afficher le résultat None, tandis que j'ai utilisé la fonction if, que je pense également fonctionner. 


**Yingzi : **

J'ai terminé le premier exo du TP-disant.

J'ai téléchargé le script donné par le prof et l'ai testé, puis j'ai trouvé le code suivant:  `import analyse_sp as analyse ` qui permet de changer l'analyseur utilisé est la ligne qui importe le module  `analyse_sp ` Si on veut utiliser un autre analyseur, on peut remplacer cette ligne par une autre ligne qui importe le module de l'analyseur de notre choix. Par exemple, si on veut utiliser l'analyseur `trankit` au lieu de `spacy`, vous pouvez utiliser : `import analyse_tk as analyse`.

Et puis, j'ai trouvé aussi le module qui permet d’afficher une barre de chargement pendant l’analyse morphosyntaxique: `from tqdm import tqdm`. `tqdm` permet d'afficher une barre de progression ainsi que des informations sur le temps écoulé et le temps restant. 

Après,  j'ai ajouté un fichier nommé: `export_pickle.py` et une ligne `from export_pickle import write_pickle` dans le script `extract_many.py` pour la gestion des pickle.






### 2023.04.18

**Xiaohua : **

Jusqu'à maintenant, nous avons obtenu des informations à partir de fichiers XML existants. Cette semaine, nous devons essayer à prédire les sujets des articles en utilisant le modèle LDA. J'ai essayé et réussi à exécuter le script de démonstration LDA. En plus, j'ai regardé une vidéo de Luis Serrano sur YouTube pour comprendre la logique générale. Cependant, comprendre pleinement la production de ce script reste difficile. 

En suivant l'exemple de fichier, j'ai créé un nouveau fichier Python pour prédire les sujets.J'ai d'abord fixé la structure globale du corpus du modèle (une liste de listes d'éléments de chaîne), j'ai donc écrit trois fonctions ((((pour les formats de données XML, JSON et pickle, avec les modules ElementTree, json et pickle, afin de les convertir en un format de données uniforme. Cela était difficile car différents types de données ont des méthodes d'importation différentes. J'ai également mis en œuvre des paramètres facultatifs pour filtrer les étiquettes POS spécifiques, et pour choisir le lemma ou la forme comme élément de corpus. 

Enfin, j'ai utilisé la bibliothèque pyLDAvis pour visualiser le modèle LDA et j'ai essayé d'expliquer le sens de chaque sujet.

Problèmes : 

1. Les utilisateurs peuvent définir des paramètres tels que le chemin du fichier, le type de données et les paramètres du modèle.  Mais je suis encore en train de déterminer les parametres  les plus idéaux. 
2. Pour des articles ou des corpus de petite taille, la ligne `dictionary.filter_extremes(no_below=5, no_above=0.9)` aura une impact fondamental sur le résultat du dictionnaire, (parfois le rendre 0 uniques tokens), donc je me demande s'il faut laisser l'utilisateur à choisir ces deux paramètres.
3. Les stopwords : Dans le corpus, il reste, après des filtrages, des mots non-senses, comme `d'`,`avec`,`non`,`au`,`des`,  je me demande aussi s'il faut ajouter une liste de stopwords pour les supprimer

```python
stop_words = ["aux", "c'", "ou", "des", ...]
docs = [[token for token in doc if token not in stop_words] for doc in docs]

```

4. Le but de cette semaine(ou du projet) reste un peu vague pour moi, par exemple, c'est quoi le cible d'analyse, les descriptions ou les titres ?
5. J'ajoute ces lignes au bout du script [pluri_extraire(avec analyse).py](https://gitlab.com/ppe2023/ppe2_lcd/-/blob/XC-s10/scripts/pluri_extraire(avec analyse).py), mais il semble que la seule façon d'obtenir les paramètres du module LDA est `input`, et ce n'est pas possible d'utiliser deux fois `argparse` dans un fichier. J'attends une meilleure solution pour lier les deux scripts.

```python
    continue_ou_non = input(
        "Do you want to start the LDA topic module ? (y/n): ")
    if continue_ou_non.lower() == 'y':
        use_saved_data = input("Do you want to use saved data? (y/n): ")
        if use_saved_data.lower() == 'y':
          num_topics = input("num_topics please:")
          chunksize = input("chunksize please:")  
          pass
        else:
          pass
```

**Yingzi : **

J'ai réalisé le premier mini-exercice de l'exercice 2 du TP-disant: fais tourner le code fourni par la page du modèle LDA.
Mais je me suis bloqué dans un deuxième petit exercice car je ne savais pas comment modifier le script pour l'intégrer à nos données. J'attends la correction du TP de prof la semaine prochaine. 


### 2023.04.20

**Yingzi : **

Après avoir consulté les corrections du prof sur le TP-distant et les avoir combinées avec la version élaborée par Xiaohua, j'ai effectué les modifications suivantes sur mon TP-distant et complété les parties que je n'avais pas terminées auparavant : 

- Ajout des bibliothèques supplémentaires : pickle, tarfile, re, os.path, io, json, et les bibliothèques nltk.
- Importation des structures de données personnalisées : Corpus, Article, Analyse.
- Modification des fonctions de chargement de fichiers : load_file_xml, load_file_json, load_file_pickle.
- Ajout du traitement des bigrams avec la fonction bigram utilisant le modèle Phrases.
- Modification de la fonction de filtrage des termes extrêmes avec la fonction filter_extremes utilisant l'objet Dictionary.
- Modification de la fonction de construction du modèle LDA avec la fonction train_lda_model utilisant LdaModel.
- Modification de l'affichage de la cohérence des sujets avec la fonction topic_coherence.
- Ajout de l'analyse des arguments de ligne de commande avec la bibliothèque argparse.
- Appel des fonctions appropriées dans la fonction principale en fonction des arguments de ligne de commande.
- Ajout d'instructions d'impression pour afficher des informations pendant l'exécution.

TP-distant est fini. Nous allons commencer à faire la mise en forme du contenu exporté.
